export interface AIConfig {
  provider: 'deepseek' | 'siliconflow' | 'ollama';
  apiKey: string;
  model?: string;
  temperature?: number;
  maxTokens?: number;
}

export interface AIMessage {
  role: 'user' | 'assistant' | 'system';
  content: string;
}

export interface AIResponse {
  content: string;
  isComplete: boolean;
}

export interface AIProviderConfig {
  name: string;
  icon: string;
  description: string;
  models: Array<{
    id: string;
    name: string;
    maxTokens: number;
  }>;
  defaultModel: string;
}

export const AI_PROVIDERS: Record<string, AIProviderConfig> = {
  deepseek: {
    name: 'Deepseek',
    icon: 'ğŸ§ ',
    description: 'ä¸“ä¸šçš„ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹',
    models: [
      { id: 'deepseek-chat', name: 'Deepseek Chat', maxTokens: 4096 },
      { id: 'deepseek-coder', name: 'Deepseek Coder', maxTokens: 8192 }
    ],
    defaultModel: 'deepseek-chat'
  },
  siliconflow: {
    name: 'ç¡…åŸºæµä½“',
    icon: 'ğŸŒŠ',
    description: 'å›½å†…é¢†å…ˆçš„ AI æœåŠ¡æä¾›å•†',
    models: [
      { id: 'siliconflow-chat', name: 'SiliconFlow Chat', maxTokens: 4096 }
    ],
    defaultModel: 'siliconflow-chat'
  },
  ollama: {
    name: 'Ollama',
    icon: 'ğŸ¦™',
    description: 'æœ¬åœ°è¿è¡Œçš„å¼€æºå¤§è¯­è¨€æ¨¡å‹',
    models: [
      { id: 'llama2', name: 'Llama 2', maxTokens: 4096 },
      { id: 'mistral', name: 'Mistral', maxTokens: 8192 },
      { id: 'codellama', name: 'Code Llama', maxTokens: 16384 }
    ],
    defaultModel: 'llama2'
  }
}; 